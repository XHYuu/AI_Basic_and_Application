{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc760ce-ac8a-479f-81d1-c91061630528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils import *\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30986819-d0d1-4b36-b581-89347f0f5479",
   "metadata": {},
   "source": [
    "##  3.1 Exploring the Dataset\n",
    "\n",
    "First, download and extract this IMDb review dataset\n",
    "in the path `./data/aclImdb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbbc773-6d02-4f50-aa8a-b6fb73def05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "data_dir = download_extract(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37172566-aab4-40bd-acf3-96f318772115",
   "metadata": {},
   "source": [
    "We'll start by loading the training data, which includes the reviews and their associated sentiment labels. After loading the data, we'll inspect a few examples to understand the format and content of the reviews and their corresponding sentiment labels. \n",
    "\n",
    "An example review might look like: \"The movie was fantastic, and I thoroughly enjoyed it.\" The corresponding label for this positive review would be: 1. \n",
    "\n",
    "Conversely, a negative review might look like: \"The plot was confusing, and the acting was subpar.\". The corresponding label for this negative review would be: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8cc32-bbfa-4359-a967-f89b4aa47756",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_imdb(data_dir, is_train=True)\n",
    "print('# trainings:', len(train_data[0]))\n",
    "for x, y in zip(train_data[0][:3], train_data[1][:3]):\n",
    "    print('label:', y, 'review:', x[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479d452-42c9-4154-8f98-20259cbf8506",
   "metadata": {},
   "source": [
    "Please complete the following function.\n",
    "\n",
    "When completing this function, consider the following:\n",
    "\n",
    "- The function should take a text input and split it into individual word tokens (where each word is treated as a separate token).\n",
    "\n",
    "In the case of word tokenization, a sentence like \"The quick brown fox\" would be split into individual tokens: [\"The\", \"quick\", \"brown\", \"fox\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d41c54-d5af-4b02-847d-e84b424f1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines):\n",
    "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b788557-acef-4e71-9a25-4dca80d258ef",
   "metadata": {},
   "source": [
    "Please plot the data distribution of the training data using a histogram. The x-axis should represent the length of each review, and the y-axis should indicate the number of corresponding samples. The x-axis will display the lengths of the reviews, while the y-axis will indicate the frequency or count of reviews with similar lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16592eb5-5287-4a07-8f7b-cec92595978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793b7fb",
   "metadata": {},
   "source": [
    "The **Vocab** class encapsulates the core operations related to vocabulary management, offering a convenient interface for working with text data in a structured manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af77174-5189-4cb8-a54c-f962cd9535ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self): \n",
    "        return self.token_to_idx['<unk>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6273a3-a081-4500-a3d7-4ecde80dbe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"Truncate or pad sequences.\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Truncate\n",
    "    return line + [padding_token] * (num_steps - len(line))  # Pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f6973-8ea1-41fb-b7ee-8a0000dd69a0",
   "metadata": {},
   "source": [
    "Please complete the **load_data_imdb** function, which aims to facilitate the processing of the IMDb review dataset by providing a data loader and the vocabulary associated with the dataset. To achieve this, the function is expected to utilize predefined functions and classes to handle the dataset and its associated vocabulary. \n",
    "\n",
    "**Note: To process a minibatch of such reviews at each time, we set the length of each review to 500 with truncation and padding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcdac4-750b-4323-a25c-8c5e63b587be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_imdb(batch_size, num_steps=500):\n",
    "    \"\"\"Return data loader and the vocabulary of the IMDb review dataset.\"\"\"\n",
    "    \n",
    "    return train_loader, test_loader, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e7f39-2d03-43fb-bc51-5d9f5e57558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = \n",
    "train_iter, test_iter, vocab = load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec9c88-a1c6-48fe-9d6a-306a85ef7a89",
   "metadata": {},
   "source": [
    "##  3.2 Using Bidirectional Recurrent Neural Networks for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd1664-0feb-4e7e-ac98-d8068fb92f6b",
   "metadata": {},
   "source": [
    "Please design a multilayer bidirectional RNN to process the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9391c4-2423-4bc1-9c0d-212c7ae7d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens,\n",
    "                 num_layers, **kwargs):\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        ### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4166fa9-1215-460a-a672-d116a748ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers =  ### YOUR CODE HERE ###\n",
    "net =   ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb60d3-7aaa-4647-a6dc-c5f57d360cdf",
   "metadata": {},
   "source": [
    "Please initialize the weights of your predefined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29681b5-76d9-4079-9057-cbb853732ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "     ### START CODE HERE ###\n",
    "\n",
    "     ### END CODE HERE ###  \n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4dcefc-e137-4c65-86b0-296883f8f944",
   "metadata": {},
   "source": [
    "Next, we will load the pretrained 100-dimensional GloVe embeddings for tokens in the vocabulary. GloVe (Global Vectors for Word Representation) provides pre-defined dense vectors for a vast number of words in the English language, allowing for immediate utilization in various natural language processing (NLP) applications. The GloVe embeddings are available in different dimensions, such as 50-d, 100-d, 200-d, or 300-d vectors, with the dimensionality indicating the size of the vector representation for each word. To incorporate these pretrained embeddings, it's crucial to ensure consistency with the specified embed_size of 100. By leveraging the GloVe embeddings, we can enrich the model's understanding of the textual data and enhance its performance in NLP tasks.\n",
    "\n",
    "You can downlaod the **glove.6b.100d** from https://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da3a08-a244-4db2-8ca9-74f0d8ed5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = TokenEmbedding('glove.6b.100d', './data')\n",
    "embeds = glove_embedding[vocab.idx_to_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a31b1a0-1070-45e7-ab1f-587584324348",
   "metadata": {},
   "source": [
    "Please use these pretrained word vectors to represent tokens in the reviews and ensure not update these vectors during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1eed8f-9903-4b2e-9179-7ab64df067a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "### END CODE HERE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3033d8-7757-4b0f-be09-a5800200616a",
   "metadata": {},
   "source": [
    "Finally, we will proceed with training our network. Within the following function, you are expected to train your network and evaluate your model on the testing dataset. Additionally, you should generate visualizations depicting the training loss, training accuracy, and testing accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf956cfa-ae87-4b07-8cdd-9f75123f0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, loss, trainer, num_epochs):\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d668a5a-0a0d-4238-94eb-90b809e93adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "train(net, train_iter, test_iter, loss, trainer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b0360-53f0-492e-9a91-cffb5aa0a834",
   "metadata": {},
   "source": [
    "##  3.3 Using TextCNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b63ba8-810a-47eb-a99e-b0e41b5c25d4",
   "metadata": {},
   "source": [
    "Please design a TextCNN to process the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434bd00-a937-4648-b9b6-6d66fd5ba208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,\n",
    "                 **kwargs):\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        ### END CODE HERE ### \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        ### END CODE HERE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78480182-3e2d-49b1-8ac9-c60628075114",
   "metadata": {},
   "source": [
    "Please initialize the weights of your predefined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d374d1e-78de-4c32-b05d-afb2a7f1b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9200259",
   "metadata": {},
   "source": [
    "Next, we will load the pretrained 100-dimensional GloVe embeddings for tokens in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = TokenEmbedding('glove.6b.100d', './data')\n",
    "embeds = glove_embedding[vocab.idx_to_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d46d57",
   "metadata": {},
   "source": [
    "Please use these pretrained word vectors to represent tokens in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "### END CODE HERE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52dcdb2-e17e-408b-b7b7-3e8ae0b0e182",
   "metadata": {},
   "source": [
    "Finally, we will proceed with training our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81817d1-b161-470d-8e77-1fe9050d4e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, kernel_sizes, nums_channels =  ### YOUR CODE HERE ###\n",
    "net = ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8be3a3-2d95-4007-95c6-fd70b72c2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "train(net, train_iter, test_iter, loss, trainer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786df46c",
   "metadata": {},
   "source": [
    "Make the prediction for a text sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b2839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sequence):\n",
    "    \"\"\"Predict the sentiment of a text sequence.\"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    ### END CODE HERE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0634f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(net, vocab, 'this movie is so great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(net, vocab, 'this movie is so bad')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
