{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: CNN for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll be coding up a convolutional neural network from scratch to classify images using PyTorch.  \n",
    "\n",
    "### Instructions\n",
    "- Install PyTorch following the instructions [here](https://pytorch.org/).\n",
    "- Install the [`torchinfo` package](https://github.com/TylerYep/torchinfo) to visualize the network architecture and the number of parameters. The maximum number of parameters you are allowed to use for your network is **100,000**. Those who violate this rule will be scored 0 points!!! \n",
    "- You are required to complete the functions defined in the code blocks following each question. Fill out sections of the code marked `\"YOUR CODE HERE\"`.\n",
    "- You're free to add any number of methods within each class.\n",
    "- You may also add any number of additional code blocks that you deem necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will use the EMNIST dataset. The EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19 and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset.\n",
    "\n",
    "There are six different splits provided in this datase: byclass, bymerge, balanced, letters, digits and mnist. Here, we will choose 'balanced' split.\n",
    "\n",
    "Further information on the dataset contents and conversion process can be found in the paper available at https://arxiv.org/abs/1702.05373v1.\n",
    "\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.  \n",
    "\n",
    "\n",
    "EMNIST is included in the `torchvision` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import EMNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to MNIST_data/EMNIST/raw/gzip.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 561753746/561753746 [03:10<00:00, 2944729.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/EMNIST/raw/gzip.zip to MNIST_data/EMNIST/raw\n"
     ]
    }
   ],
   "source": [
    "# Transform to normalize the data and convert to a tensor\n",
    "transform = Compose([ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "# Download the data\n",
    "dataset = EMNIST('MNIST_data/', download = True, train = True, split = 'balanced', transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** You may add more operations to `Compose` if you're performing data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the classes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'd', 'e', 'f', 'g', 'h', 'n', 'q', 'r', 't']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import matplotlib.pyplot as plt\n",
    "\n",
    "def show_example(img, label):\n",
    "    print('Label: {} ({})'.format(dataset.classes[label], label))\n",
    "    plt.imshow(img.squeeze(), cmap='Greys_r')\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: h (42)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(*dataset[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: A (10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(*dataset[20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (2 points)\n",
    "\n",
    "## Creating Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split_indices` function takes in the size of the entire dataset, `n`, the fraction of data to be used as validation set, `val_frac`, and the random seed and returns the indices of the data points to be added to the validation dataset.  \n",
    "\n",
    "**Choose a suitable fraction for your validation set and experiment with the seed. Remember that the better your validation set, the higher the chances that your model would do well on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(n, val_frac, seed):\n",
    "    # Determine the size of the validation set\n",
    "    n_val = int(val_frac * n)\n",
    "    np.random.seed(seed)\n",
    "    # Create random permutation between 0 to n-1\n",
    "    idxs = np.random.permutation(n)\n",
    "    # Pick first n_val indices for validation set\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples in training set: 56400\n",
      "#samples in validation set: 56400\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#   YOUR CODE HERE   #\n",
    "######################\n",
    "val_frac = 0.5 ## Set the fraction for the validation set\n",
    "rand_seed = 42 ## Set the random seed\n",
    "\n",
    "train_indices, val_indices = split_indices(len(dataset), val_frac, rand_seed)\n",
    "print(\"#samples in training set: {}\".format(len(train_indices)))\n",
    "print(\"#samples in validation set: {}\".format(len(val_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make use of the built-in dataloaders in PyTorch to create iterables of our our training and validation sets. This helps in avoiding fitting the whole dataset into memory and only loads a batch of the data that we can decide. \n",
    "\n",
    "**Set the `batch_size` depending on the hardware resource (GPU/CPU RAM) you are using for the assignment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#   YOUR CODE HERE   #\n",
    "######################\n",
    "batch_size = 8 ## Set the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sampler and data loader\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_dl = DataLoader(dataset,\n",
    "                     batch_size,\n",
    "                     sampler=train_sampler)\n",
    "\n",
    "# Validation sampler and data loader\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_dl = DataLoader(dataset,\n",
    "                   batch_size,\n",
    "                   sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot images in a sample batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(10,10))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images, 8).permute(1, 2, 0), cmap='Greys_r')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (10 points)\n",
    "\n",
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create your model by defining the network architecture in the `ImageClassifierNet` class.**  \n",
    "**NOTE:** The number of parameters in your network must be $\\leq$ 100,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierNet(nn.Module):\n",
    "    def __init__(self, n_channels=1):\n",
    "        super(ImageClassifierNet, self).__init__()\n",
    "        ######################\n",
    "        #   YOUR CODE HERE   #\n",
    "        ######################\n",
    "        self.conv1 = nn.Conv2d(n_channels, 16, kernel_size=3, stride=1, padding=1) \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2) \n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) \n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 48, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(48 * 3 * 3, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 47)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        ######################\n",
    "        #   YOUR CODE HERE   #\n",
    "        ######################\n",
    "        x = F.relu(self.pool1(self.bn1(self.conv1(X))))\n",
    "        x = F.relu(self.pool2(self.bn2(self.conv2(x))))\n",
    "        x = F.relu(self.pool3(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc2(F.relu(self.fc1(x)))\n",
    "        return self.fc3(F.relu(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageClassifierNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block prints your network architecture. It also shows the total number of parameters in your network (see `Total params`).  \n",
    "\n",
    "**NOTE: The total number of parameters in your model should be <= 100,000.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ImageClassifierNet                       [8, 47]                   --\n",
       "├─Conv2d: 1-1                            [8, 16, 28, 28]           160\n",
       "├─BatchNorm2d: 1-2                       [8, 16, 28, 28]           32\n",
       "├─MaxPool2d: 1-3                         [8, 16, 14, 14]           --\n",
       "├─Conv2d: 1-4                            [8, 32, 14, 14]           4,640\n",
       "├─BatchNorm2d: 1-5                       [8, 32, 14, 14]           64\n",
       "├─MaxPool2d: 1-6                         [8, 32, 7, 7]             --\n",
       "├─Conv2d: 1-7                            [8, 48, 7, 7]             13,872\n",
       "├─BatchNorm2d: 1-8                       [8, 48, 7, 7]             96\n",
       "├─MaxPool2d: 1-9                         [8, 48, 3, 3]             --\n",
       "├─Linear: 1-10                           [8, 128]                  55,424\n",
       "├─Linear: 1-11                           [8, 64]                   8,256\n",
       "├─Linear: 1-12                           [8, 47]                   3,055\n",
       "==========================================================================================\n",
       "Total params: 85,599\n",
       "Trainable params: 85,599\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 14.25\n",
       "==========================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 2.72\n",
       "Params size (MB): 0.34\n",
       "Estimated Total Size (MB): 3.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(batch_size, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable training on a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This section is necessary if you're training your model on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Use GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierNet(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=432, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=47, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (10 points)\n",
    "\n",
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the `train_model` function to train your model on a dataset. Tune your network architecture and hyperparameters on the validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs, model, train_dl, val_dl, loss_fn, opt_fn, lr):\n",
    "    \"\"\"\n",
    "    Trains the model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        n_epochs: number of epochs\n",
    "        model: ImageClassifierNet object\n",
    "        train_dl: training dataloader\n",
    "        val_dl: validation dataloader\n",
    "        loss_fn: the loss function\n",
    "        opt_fn: the optimizer\n",
    "        lr: learning rate\n",
    "    \n",
    "    Returns:\n",
    "        The trained model. \n",
    "        A tuple of (model, train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "    \"\"\"\n",
    "    # Record these values the end of each epoch\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "    \n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    optimizer = opt_fn(model.parameters(), lr=lr)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "\n",
    "        epoch_train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for img, label in tqdm(train_dl, desc=f\"Training process in epoch {epoch}\"):\n",
    "            output = model(img)\n",
    "            loss = loss_fn(output, label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_train += (predicted == label).sum().item()\n",
    "            total_train += label.size(0)\n",
    "            \n",
    "        train_loss = epoch_train_loss / len(train_dl)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        \n",
    "        epoch_val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for img, label in tqdm(val_dl, desc=f\"Testing process in epoch {epoch}\"):\n",
    "                \n",
    "                output = model(img)\n",
    "                loss = loss_fn(output, label)\n",
    "                \n",
    "                epoch_val_loss += loss.item()\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                correct_val += (predicted == label).sum().item()\n",
    "                total_val += label.size(0)\n",
    "        \n",
    "        val_loss = epoch_val_loss / len(val_dl)\n",
    "        val_accuracy = correct_val / total_val\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    return model, train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the maximum number of training epochs, the loss function, the optimizer, and the learning rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#   YOUR CODE HERE   #\n",
    "######################\n",
    "num_epochs = 20  # Max number of training epochs\n",
    "loss_fn = loss_fn = torch.nn.CrossEntropyLoss() # Define the loss function\n",
    "opt_fn = torch.optim.Adam # Select an optimizer\n",
    "lr = 0.01 # Set the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:43<00:00, 161.31it/s]\n",
      "Testing process in epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:19<00:00, 367.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:\n",
      "  Train Loss: 0.7160, Train Accuracy: 0.7673\n",
      "  Val Loss: 0.8896, Val Accuracy: 0.7588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:42<00:00, 164.97it/s]\n",
      "Testing process in epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:19<00:00, 361.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:\n",
      "  Train Loss: 0.6946, Train Accuracy: 0.7770\n",
      "  Val Loss: 0.7093, Val Accuracy: 0.7745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 2: 100%|█████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:43<00:00, 161.94it/s]\n",
      "Testing process in epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:19<00:00, 359.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:\n",
      "  Train Loss: 0.6779, Train Accuracy: 0.7802\n",
      "  Val Loss: 0.7880, Val Accuracy: 0.7824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 3: 100%|█████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:43<00:00, 163.47it/s]\n",
      "Testing process in epoch 3: 100%|██████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:19<00:00, 369.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:\n",
      "  Train Loss: 0.6589, Train Accuracy: 0.7870\n",
      "  Val Loss: 0.6692, Val Accuracy: 0.7939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 4: 100%|█████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:42<00:00, 164.93it/s]\n",
      "Testing process in epoch 4: 100%|██████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:19<00:00, 371.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:\n",
      "  Train Loss: 0.6490, Train Accuracy: 0.7894\n",
      "  Val Loss: 0.6260, Val Accuracy: 0.8061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 5: 100%|█████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:42<00:00, 166.60it/s]\n",
      "Testing process in epoch 5: 100%|██████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:18<00:00, 379.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:\n",
      "  Train Loss: 0.6459, Train Accuracy: 0.7907\n",
      "  Val Loss: 0.7644, Val Accuracy: 0.7635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 6: 100%|█████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:42<00:00, 165.96it/s]\n",
      "Testing process in epoch 6: 100%|██████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:18<00:00, 373.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:\n",
      "  Train Loss: 0.6398, Train Accuracy: 0.7918\n",
      "  Val Loss: 0.7569, Val Accuracy: 0.7770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 7: 100%|█████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:42<00:00, 165.90it/s]\n",
      "Testing process in epoch 7: 100%|██████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:19<00:00, 368.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:\n",
      "  Train Loss: 0.6365, Train Accuracy: 0.7941\n",
      "  Val Loss: 0.6775, Val Accuracy: 0.7929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 8: 100%|█████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:42<00:00, 166.07it/s]\n",
      "Testing process in epoch 8: 100%|██████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:18<00:00, 377.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:\n",
      "  Train Loss: 0.6352, Train Accuracy: 0.7948\n",
      "  Val Loss: 0.7317, Val Accuracy: 0.7659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:42<00:00, 165.68it/s]\n",
      "Testing process in epoch 9: 100%|██████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:18<00:00, 375.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:\n",
      "  Train Loss: 0.6279, Train Accuracy: 0.7973\n",
      "  Val Loss: 0.6732, Val Accuracy: 0.8034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 10: 100%|████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:42<00:00, 165.48it/s]\n",
      "Testing process in epoch 10: 100%|█████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:19<00:00, 368.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:\n",
      "  Train Loss: 0.6253, Train Accuracy: 0.7992\n",
      "  Val Loss: 0.6485, Val Accuracy: 0.7967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 11: 100%|████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:45<00:00, 155.69it/s]\n",
      "Testing process in epoch 11: 100%|█████████████████████████████████████████████████████████████████████████████| 7050/7050 [00:20<00:00, 346.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20:\n",
      "  Train Loss: 0.6280, Train Accuracy: 0.7999\n",
      "  Val Loss: 0.7141, Val Accuracy: 0.7914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process in epoch 12:  29%|██████████████████████▎                                                     | 2067/7050 [00:13<00:30, 161.97it/s]"
     ]
    }
   ],
   "source": [
    "history = train_model(num_epochs, model, train_dl, val_dl, loss_fn, opt_fn, lr)\n",
    "model, train_losses, val_losses, train_accuracies, val_accuracies = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(train_accuracies, val_accuracies):\n",
    "    \"\"\"Plot accuracies\"\"\"\n",
    "    plt.plot(train_accuracies, \"-x\")\n",
    "    plt.plot(val_accuracies, \"-o\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    plt.title(\"Accuracy vs. No. of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    \"\"\"Plot losses\"\"\"\n",
    "    plt.plot(train_losses, \"-x\")\n",
    "    plt.plot(val_losses, \"-o\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    plt.title(\"Loss vs. No. of Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, _ = split_indices(len(dataset), 0, rand_seed)\n",
    "\n",
    "sampler = SubsetRandomSampler(indices)\n",
    "dl = DataLoader(dataset, batch_size, sampler=sampler)\n",
    "dl = DeviceDataLoader(dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the maximum number of training epochs and the learning rate for finetuning your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#   YOUR CODE HERE   #\n",
    "######################\n",
    "num_epochs = 20 # Max number of training epochs\n",
    "lr = 0.01 # Set the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(num_epochs, model, dl, [], loss_fn, opt_fn, lr)\n",
    "model = history[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_prediction(img, label, probs, classes):\n",
    "    \"\"\"\n",
    "    Visualize predictions.\n",
    "    \"\"\"\n",
    "    probs = probs.cpu().numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(8,15), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).cpu().numpy().squeeze(), cmap='Greys_r')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Actual: {}'.format(classes[label]))\n",
    "    ax2.barh(np.arange(10), probs)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(classes, size='small');\n",
    "    ax2.set_title('Predicted: probabilities')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the class probabilites (log softmax) for img\n",
    "images = iter(dl)\n",
    "for imgs, labels in images:\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Calculate the class probabilites (log softmax) for img\n",
    "        probs = torch.nn.functional.softmax(model(imgs[0].unsqueeze(0)), dim=1)\n",
    "        # Plot the image and probabilites\n",
    "        view_prediction(imgs[0], labels[0], probs, dataset.classes)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very important\n",
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (8 points)\n",
    "\n",
    "## Compute accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = EMNIST('MNIST_data/', download = True, train = False, split = 'balanced', transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_dataset, batch_size)\n",
    "test_dl = DeviceDataLoader(test_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dl):\n",
    "    \"\"\"\n",
    "    Evaluates your model on the test data.\n",
    "    \n",
    "    Args:\n",
    "        model: ImageClassifierNet object\n",
    "        test_dl: test dataloader\n",
    "    \n",
    "    Returns: \n",
    "        Test accuracy.\n",
    "    \"\"\"\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    model.eval()\n",
    "\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in val_dl:\n",
    "            output = model(img)\n",
    "            \n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_val += (predicted == label).sum().item()\n",
    "            total_val += label.size(0)\n",
    "    \n",
    "    return correct_val / total_val\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Accuracy = {:.4f}\".format(evaluate(model, test_dl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips to increase the test accuracy\n",
    "\n",
    "- **Data augmentation:** Diversifies your training set and leads to better generalization\n",
    "    - Flipping\n",
    "    - Rotation\n",
    "    - Shifting\n",
    "    - Cropping\n",
    "    - Adding noise\n",
    "    - Blurring\n",
    "    \n",
    "- **Regularization:** Reduces overfitting on the training set\n",
    "    - Early stopping\n",
    "    - Dropout\n",
    "    - $l_2$ regularization\n",
    "    - Batch normalization\n",
    "\n",
    "- **Hyperparameter tuning:**\n",
    "    - Weight initialization\n",
    "    - Learning rate\n",
    "    - Activation functions\n",
    "    - Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
