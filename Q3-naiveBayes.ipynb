{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIN7102 Applied Data Mining and Text Analytics: Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Naive Bayes implementation.\n",
    "code borrowed from: https://faculty.wcas.northwestern.edu/robvoigt/courses/2023_spring/ling334/assignments/a3.html\n",
    "\n",
    "API inspired by SciKit-learn.\n",
    "\"\"\"\n",
    "\n",
    "import os, math\n",
    "from collections import Counter\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    \"\"\"Code for a bag-of-words Naive Bayes classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_dir='data/haiti/train', REMOVE_STOPWORDS=False):\n",
    "        self.REMOVE_STOPWORDS = REMOVE_STOPWORDS\n",
    "        self.stopwords = set([l.strip() for l in open('data/english.stop')])\n",
    "        self.classes = os.listdir(train_dir)\n",
    "        self.train_data = {c: os.path.join(train_dir, c) for c in self.classes}\n",
    "        self.vocabulary = set([])\n",
    "        self.logprior = {}\n",
    "        self.loglikelihood = {} # keys should be tuples in the form (w, c)\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Train the Naive Bayes classification model, following the pseudocode for\n",
    "        training given in Figure 4.2 of SLP Chapter 4 (https://web.stanford.edu/~jurafsky/slp3/4.pdf). \n",
    "\n",
    "        Note that self.train_data contains the paths to training data files. \n",
    "        To get all the documents for a given training class c in a list, you can use:\n",
    "            c_docs = open(self.train_data[c]).readlines()\n",
    "\n",
    "        The dataset's are pre-tokenized so you can get words with\n",
    "        simply `words = doc.split()`\n",
    "\n",
    "        Remember to account for whether the self.REMOVE_STOPWORDS flag is set or not;\n",
    "        if it is True then the stopwords in self.stopwords should be removed whenever\n",
    "        they appear.\n",
    "\n",
    "        When converting from the pseudocode, consider how many loops over the data you\n",
    "        will need to properly estimate the parameters of the model, and what intermediary\n",
    "        variables you will need in this function to store the results of your computations.\n",
    "\n",
    "        Follow the TrainNaiveBayes pseudocode to update the relevant class variables: \n",
    "            - self.vocabulary, self.logprior, and self.loglikelihood. \n",
    "        \n",
    "        Note that the keys for self.loglikelihood should be tuples in the form of (w, c) \n",
    "        where w is the string for the word and c is the string for the class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None (reads training data from self.train_data)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None (updates class attributes self.vocabulary, self.logprior, self.loglikelihood)\n",
    "        \"\"\"\n",
    "        # >>> YOUR ANSWER HERE\n",
    "\n",
    "        # Calculate class priors\n",
    "\n",
    "        # Calculate likelihoods\n",
    "\n",
    "        # >>> END YOUR ANSWER\n",
    "        \n",
    "    def score(self, doc, c):\n",
    "        \"\"\"Return the log-probability of a given document for a given class,\n",
    "        using the trained Naive Bayes classifier. \n",
    "\n",
    "        This is analogous to the inside of the for loop in the TestNaiveBayes\n",
    "        pseudocode in Figure 4.2, SLP Chapter 4 (https://web.stanford.edu/~jurafsky/slp3/4.pdf).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        doc : str\n",
    "            The text of a document to score.\n",
    "        c : str\n",
    "            The name of the class to score it against.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The log-probability of the document under the model for class c.\n",
    "        \"\"\"        \n",
    "        # >>> YOUR ANSWER HERE\n",
    "        return 0.0\n",
    "        # >>> END YOUR ANSWER\n",
    "                \n",
    "    def predict(self, doc):\n",
    "        \"\"\"Return the most likely class for a given document under the trained classifier model.\n",
    "        This should be only a few lines of code, and should make use of your self.score function.\n",
    "\n",
    "        Consider using the `max` built-in function. There are a number of ways to do this:\n",
    "           https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        doc : str\n",
    "            A text representation of a document to score.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The most likely class as predicted by the model.\n",
    "        \"\"\"\n",
    "        # >>> YOUR ANSWER HERE\n",
    "        return predicted_class\n",
    "        # >>> END YOUR ANSWER\n",
    "\n",
    "\n",
    "    def evaluate(self, test_dir='haiti/test', target='relevant'):\n",
    "        \"\"\"Calculate a precision, recall, and F1 score for the model\n",
    "        on a given test set.\n",
    "\n",
    "        Not the 'target' parameter here, giving the name of the class\n",
    "        to calculate relative to. So you can consider a True Positive\n",
    "        to be an instance where the gold label for the document is the\n",
    "        target and the model also predicts that label; a False Positive\n",
    "        to be an instance where the gold label is *not* the target, but\n",
    "        the model predicts that it is; and so on.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_dir : str\n",
    "            The path to a directory containing the test data.\n",
    "        target : str\n",
    "            The name of the class to calculate relative to. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (float, float, float)\n",
    "            The model's precision, recall, and F1 score relative to the\n",
    "            target class.\n",
    "        \"\"\"        \n",
    "        test_data = {c: os.path.join(test_dir, c) for c in self.classes}\n",
    "        if not target in test_data:\n",
    "            print('Error: target class does not exist in test data.')\n",
    "            return\n",
    "        outcomes = {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0}\n",
    "        # >>> YOUR ANSWER HERE\n",
    "        # Calculate outcomes\n",
    "\n",
    "        # calculate precision, recall, and F1 score\n",
    "\n",
    "        # >>> END YOUR ANSWER\n",
    "        return (precision, recall, f1_score)\n",
    "\n",
    "\n",
    "    def print_top_features(self, k=10):\n",
    "        results = {c: {} for c in self.classes}\n",
    "        for w in self.vocabulary:\n",
    "            for c in self.classes:\n",
    "                ratio = math.exp(self.loglikelihood[w, c] - min(self.loglikelihood[w, other_c] for other_c in self.classes if other_c != c) )\n",
    "                results[c][w] = ratio\n",
    "\n",
    "        for c in self.classes:\n",
    "            print(f'Top features for class <{c.upper()}>')\n",
    "            for w, ratio in sorted(results[c].items(), key = lambda x: x[1], reverse=True)[0:k]:\n",
    "                print(f'\\t{w}\\t{ratio}')\n",
    "            print('')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evaluate your own Naive bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'relevant'\n",
    "\n",
    "clf = NaiveBayesClassifier(train_dir = 'data/haiti/train')\n",
    "clf.train()\n",
    "print(f'Performance on class <{target.upper()}>, keeping stopwords')\n",
    "precision, recall, f1_score = clf.evaluate(test_dir = 'data/haiti/dev', target = target)\n",
    "print(f'\\tPrecision: {precision}\\t Recall: {recall}\\t F1: {f1_score}\\n')\n",
    "\n",
    "clf = NaiveBayesClassifier(train_dir = 'data/haiti/train', REMOVE_STOPWORDS=True)\n",
    "clf.train()\n",
    "print(f'Performance on class <{target.upper()}>, removing stopwords')\n",
    "precision, recall, f1_score = clf.evaluate(test_dir = 'data/haiti/dev', target = target)\n",
    "print(f'\\tPrecision: {precision}\\t Recall: {recall}\\t F1: {f1_score}\\n')\n",
    "\n",
    "\n",
    "clf.print_top_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
